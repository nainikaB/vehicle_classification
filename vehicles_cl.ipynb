{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "019ab4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import pandas as pd \n",
    "\n",
    "img_width, img_height = 224, 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf8139e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'vehicles/train'\n",
    "validation_data_dir = 'vehicles/test'\n",
    "nb_train_samples =480\n",
    "nb_validation_samples = 144\n",
    "epochs = 10\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f46199d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#channel_first activates the theano backend\n",
    "if K.image_data_format() == 'channels_first':\n",
    "\tinput_shape = (3, img_width, img_height)\n",
    "else:\n",
    "\tinput_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12571d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected relu because it overcomes the limitation faced by sigmoid and tanh\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (2, 2), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da06a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "\t\t\toptimizer='rmsprop',\n",
    "\t\t\tmetrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a633c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 480 images belonging to 2 classes.\n",
      "Found 144 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-e400c965663a>:21: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 21s 790ms/step - loss: 0.9810 - accuracy: 0.6146 - val_loss: 0.4806 - val_accuracy: 0.8071\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 19s 761ms/step - loss: 0.4420 - accuracy: 0.8188 - val_loss: 0.4546 - val_accuracy: 0.7929\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 19s 767ms/step - loss: 0.3443 - accuracy: 0.8687 - val_loss: 0.3730 - val_accuracy: 0.8286\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 19s 760ms/step - loss: 0.2873 - accuracy: 0.8979 - val_loss: 0.2494 - val_accuracy: 0.9214\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 19s 783ms/step - loss: 0.3104 - accuracy: 0.8792 - val_loss: 0.2520 - val_accuracy: 0.9143\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 20s 800ms/step - loss: 0.2402 - accuracy: 0.9187 - val_loss: 0.2169 - val_accuracy: 0.9286\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 19s 770ms/step - loss: 0.2553 - accuracy: 0.9208 - val_loss: 0.2103 - val_accuracy: 0.9286\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2476 - accuracy: 0.9125 - val_loss: 0.2047 - val_accuracy: 0.9214\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 22s 900ms/step - loss: 0.2020 - accuracy: 0.9250 - val_loss: 0.2214 - val_accuracy: 0.9214\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 21s 856ms/step - loss: 0.1881 - accuracy: 0.9104 - val_loss: 0.1896 - val_accuracy: 0.9429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f826853adc0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "\trescale=1. / 255,\n",
    "\tshear_range=0.2,\n",
    "\tzoom_range=0.2,\n",
    "\thorizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "\ttrain_data_dir,\n",
    "\ttarget_size=(img_width, img_height),\n",
    "\tbatch_size=batch_size,\n",
    "\tclass_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "\tvalidation_data_dir,\n",
    "\ttarget_size=(img_width, img_height),\n",
    "\tbatch_size=batch_size,\n",
    "\tclass_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "\ttrain_generator,\n",
    "\tsteps_per_epoch=nb_train_samples // batch_size,\n",
    "\tepochs=epochs,\n",
    "\tvalidation_data=validation_generator,\n",
    "\tvalidation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3004d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_saved2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ab1eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    " #file = open('model_saved.h5', 'w', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcb7b3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='model_saved.h5' mode='w' encoding='utf-8'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdd606c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8269b5b310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "Predicted Class (0 - bikes , 1- cars):  0.92924714\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('model_saved2.h5')\n",
    "\n",
    "image = load_img('vehicles/test/cars/4_.jpg', target_size=(224, 224))\n",
    "img = np.array(image)\n",
    "img = img / 255.0\n",
    "img = img.reshape(1,224,224,3)\n",
    "label = model.predict(img)\n",
    "print(\"Predicted Class (0 - bikes , 1- cars): \", label[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d1256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
